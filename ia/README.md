# IA - MÃ³dulo de Inteligencia Artificial

Este mÃ³dulo integra servicios de IA avanzada incluyendo Google Gemini, LangChain y LangGraph para procesamiento de lenguaje natural y automatizaciÃ³n inteligente.

## ğŸ“ Estructura

```
ia/
â”œâ”€â”€ __init__.py              # InicializaciÃ³n del mÃ³dulo
â”œâ”€â”€ README.md               # Esta documentaciÃ³n
â””â”€â”€ services/               # Servicios de IA
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ gemini_service.py   # Servicio Google Gemini
    â””â”€â”€ langgraph_service.py # Servicio LangGraph/LangChain
```

## ğŸš€ Inicio RÃ¡pido

### Importar Servicios

```python
from ia.services import gemini_service
from ia.services.langgraph_service import LangGraphService

# Usar Gemini para respuestas rÃ¡pidas
respuesta = gemini_service.generar_respuesta("Â¿CÃ³mo crear una tarea?")

# Usar LangGraph para flujos complejos (opcional)
try:
    langgraph = LangGraphService()
    resultado = langgraph.procesar_consulta_compleja("Analiza mis tareas pendientes")
except ImportError:
    print("LangGraph no disponible - usando Gemini como fallback")
```

## ğŸ¤– Servicios Disponibles

### GeminiService

Servicio principal de IA usando Google Gemini para procesamiento de lenguaje natural.

#### ConfiguraciÃ³n

```python
from ia.services.gemini_service import GeminiService

# InicializaciÃ³n automÃ¡tica con configuraciÃ³n del sistema
service = GeminiService()

# O configuraciÃ³n personalizada
service = GeminiService(
    model="gemini-2.0-flash",
    temperature=0.0,
    max_tokens=100000
)
```

#### MÃ©todos Principales

```python
# Generar respuesta simple
respuesta = gemini_service.generar_respuesta(
    prompt="Explica quÃ© es una tarea",
    contexto="Sistema de gestiÃ³n de tareas"
)

# AnÃ¡lisis de texto
analisis = gemini_service.analizar_texto(
    "Crear tarea urgente: revisar cÃ³digo",
    tipo_analisis="extraccion_entidades"
)

# Generar contenido estructurado
contenido = gemini_service.generar_contenido_estructurado(
    template="resumen_tareas",
    datos={"tareas": lista_tareas}
)
```

### LangGraphService (Opcional)

Servicio avanzado para flujos de trabajo complejos con LangChain y LangGraph.

#### InstalaciÃ³n de Dependencias

```bash
# Instalar dependencias opcionales de LangChain
pip install langchain-google-genai langgraph langchain
```

#### Uso BÃ¡sico

```python
from ia.services.langgraph_service import LangGraphService

try:
    # Inicializar servicio
    langgraph = LangGraphService()
    
    # Procesar consulta compleja
    resultado = langgraph.procesar_consulta_compleja(
        "Analiza mis tareas por prioridad y sugiere optimizaciones"
    )
    
    # Generar plan de trabajo
    plan = langgraph.generar_plan_trabajo(
        objetivo="Completar proyecto antes del viernes",
        tareas_disponibles=lista_tareas
    )
    
except ImportError:
    print("âš ï¸ LangGraph no disponible - usando Gemini como fallback")
```

## ğŸ’¡ Ejemplos de Uso

### AnÃ¡lisis de Mensajes de Usuario

```python
def procesar_mensaje_usuario(mensaje: str) -> dict:
    """Analizar mensaje del usuario y extraer intenciÃ³n"""
    
    # Usar Gemini para anÃ¡lisis
    prompt = f"""
    Analiza este mensaje del usuario y extrae:
    1. IntenciÃ³n (crear_tarea, consultar_tareas, etc.)
    2. Entidades mencionadas (fechas, prioridades, proyectos)
    3. ParÃ¡metros para la acciÃ³n
    
    Mensaje: "{mensaje}"
    
    Responde en formato JSON.
    """
    
    respuesta = gemini_service.generar_respuesta(prompt)
    return json.loads(respuesta)

# Ejemplo de uso
resultado = procesar_mensaje_usuario(
    "Crear una tarea urgente para revisar el cÃ³digo del proyecto web antes del viernes"
)
# Resultado: {
#     "intencion": "crear_tarea",
#     "entidades": {
#         "prioridad": "urgente",
#         "descripcion": "revisar el cÃ³digo",
#         "proyecto": "proyecto web",
#         "fecha_limite": "viernes"
#     }
# }
```

### GeneraciÃ³n de ResÃºmenes Inteligentes

```python
def generar_resumen_tareas(tareas: list) -> str:
    """Generar resumen inteligente de tareas"""
    
    # Preparar contexto
    contexto_tareas = "\n".join([
        f"- {t.nombre} (Prioridad: {t.prioridad}, Estado: {t.estado})"
        for t in tareas
    ])
    
    prompt = f"""
    Genera un resumen ejecutivo de estas tareas:
    
    {contexto_tareas}
    
    Incluye:
    - EstadÃ­sticas generales
    - Tareas mÃ¡s urgentes
    - Recomendaciones de priorizaciÃ³n
    
    Usa emojis para hacerlo mÃ¡s visual.
    """
    
    return gemini_service.generar_respuesta(prompt)

# Uso en bot de Telegram
async def comando_resumen(update, context):
    tareas = tareas_service.obtener_todas_las_tareas()
    resumen = generar_resumen_tareas(tareas)
    await update.message.reply_text(resumen)
```

### Asistente Conversacional

```python
class AsistenteIA:
    """Asistente conversacional inteligente"""
    
    def __init__(self):
        self.historial = []
        
    def procesar_conversacion(self, mensaje_usuario: str) -> str:
        """Procesar mensaje en contexto conversacional"""
        
        # Agregar al historial
        self.historial.append(f"Usuario: {mensaje_usuario}")
        
        # Construir contexto completo
        contexto = "\n".join(self.historial[-5:])  # Ãšltimos 5 mensajes
        
        prompt = f"""
        Eres ELiaS, un asistente inteligente para gestiÃ³n de tareas.
        
        Contexto de la conversaciÃ³n:
        {contexto}
        
        Responde de manera natural y Ãºtil. Si el usuario quiere:
        - Crear tareas: pide los detalles necesarios
        - Consultar informaciÃ³n: proporciona datos claros
        - Ayuda: explica las funcionalidades disponibles
        
        Asistente:
        """
        
        respuesta = gemini_service.generar_respuesta(prompt)
        self.historial.append(f"Asistente: {respuesta}")
        
        return respuesta

# Uso en handlers de Telegram
asistente = AsistenteIA()

async def handle_mensaje_natural(update, context):
    mensaje = update.message.text
    respuesta = asistente.procesar_conversacion(mensaje)
    await update.message.reply_text(respuesta)
```

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno

```env
# Gemini AI (requerido)
GOOGLE_API_KEY=your_api_key_here

# ConfiguraciÃ³n del modelo (opcional)
GEMINI_MODEL=gemini-2.0-flash
GEMINI_TEMPERATURE=0.0
GEMINI_MAX_TOKENS=100000

# LangChain (opcional)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langchain_api_key

# ConfiguraciÃ³n de cache (opcional)
ENABLE_AI_CACHE=true
AI_CACHE_TTL=3600
```

### PersonalizaciÃ³n de Modelos

```python
# Configurar Gemini personalizado
from ia.services.gemini_service import GeminiService

gemini_custom = GeminiService(
    model="gemini-2.0-flash",
    temperature=0.7,  # MÃ¡s creativo
    max_tokens=50000,
    safety_settings={
        "HARM_CATEGORY_HARASSMENT": "BLOCK_NONE",
        "HARM_CATEGORY_HATE_SPEECH": "BLOCK_NONE"
    }
)
```

## ğŸ§ª Testing y Debugging

### Verificar Servicios de IA

```python
# Test bÃ¡sico de Gemini
def test_gemini():
    try:
        respuesta = gemini_service.generar_respuesta("Hola, Â¿funcionas?")
        print(f"âœ… Gemini funciona: {respuesta}")
        return True
    except Exception as e:
        print(f"âŒ Error en Gemini: {e}")
        return False

# Test de LangGraph (opcional)
def test_langgraph():
    try:
        from ia.services.langgraph_service import LangGraphService
        service = LangGraphService()
        print("âœ… LangGraph disponible")
        return True
    except ImportError:
        print("âš ï¸ LangGraph no instalado")
        return False
```

### Ejecutar Tests

```bash
# Test especÃ­fico de IA
python -c "from ia.services import gemini_service; print(gemini_service.generar_respuesta('Test'))"

# Tests completos del sistema
python run_tests.py telegram  # Incluye tests de IA
```

## ğŸ“Š Monitoreo y Logging

### Habilitar Logs Detallados

```python
import logging

# Configurar logging para IA
logging.getLogger('ia').setLevel(logging.DEBUG)
logging.getLogger('google.generativeai').setLevel(logging.INFO)

# Logs de requests a APIs
logging.getLogger('httpx').setLevel(logging.INFO)
```

### MÃ©tricas de Uso

```python
from ia.services.gemini_service import get_usage_stats

# Obtener estadÃ­sticas de uso
stats = get_usage_stats()
print(f"Tokens usados hoy: {stats.get('tokens_used', 0)}")
print(f"Requests realizados: {stats.get('requests_count', 0)}")
```

## âš ï¸ Limitaciones y Consideraciones

### LÃ­mites de API

- **Gemini API**: LÃ­mites de requests por minuto y por dÃ­a
- **Tokens**: LÃ­mite de tokens por request y por mes
- **Rate Limiting**: Implementar retry logic para requests fallidos

### Costos

- **Gemini**: Facturado por tokens de entrada y salida
- **LangChain**: Algunos servicios pueden tener costos adicionales
- **Monitoreo**: Importante trackear uso para controlar costos

### Fallbacks y Robustez

```python
def generar_respuesta_robusta(prompt: str) -> str:
    """Generar respuesta with fallbacks"""
    
    try:
        # Intentar Gemini primero
        return gemini_service.generar_respuesta(prompt)
    except Exception as e:
        print(f"âš ï¸ Gemini fallÃ³: {e}")
        
        try:
            # Fallback a LangGraph si estÃ¡ disponible
            langgraph = LangGraphService()
            return langgraph.procesar_consulta_simple(prompt)
        except:
            # Ãšltimo fallback: respuesta genÃ©rica
            return "âš ï¸ Servicio de IA temporalmente no disponible. Intenta mÃ¡s tarde."
```

## ğŸ”’ Seguridad y Privacidad

### Buenas PrÃ¡cticas

- âœ… No enviar informaciÃ³n sensible a APIs externas
- âœ… Filtrar contenido antes de procesarlo
- âœ… Implementar timeouts para evitar requests colgados
- âœ… Validar respuestas de IA antes de usarlas

### Filtrado de Contenido

```python
def es_contenido_seguro(texto: str) -> bool:
    """Verificar que el contenido es seguro para procesar"""
    
    # Lista de patrones a evitar
    patrones_prohibidos = [
        r'password.*[:=]\s*\w+',
        r'token.*[:=]\s*\w+',
        r'api[_-]?key.*[:=]\s*\w+'
    ]
    
    import re
    for patron in patrones_prohibidos:
        if re.search(patron, texto, re.IGNORECASE):
            return False
    
    return True

# Usar en procesamiento
def procesar_mensaje_seguro(mensaje: str) -> str:
    if not es_contenido_seguro(mensaje):
        return "âš ï¸ No puedo procesar contenido que pueda contener informaciÃ³n sensible."
    
    return gemini_service.generar_respuesta(mensaje)
```

## ğŸ“š Referencias

- [Google AI Documentation](https://ai.google.dev/docs)
- [LangChain Documentation](https://docs.langchain.com/)
- [LangGraph Documentation](https://docs.langchain.com/langgraph)
- [Gemini API Reference](https://ai.google.dev/api)

## ğŸ¤ Contribuir

### Agregar Nuevo Servicio de IA

1. Crear archivo en `services/nuevo_servicio.py`
2. Implementar interfaz estÃ¡ndar
3. Agregar tests correspondientes
4. Actualizar documentaciÃ³n

### Optimizar Prompts

1. Crear templates reutilizables
2. A/B testing de diferentes enfoques
3. MÃ©tricas de calidad de respuestas
4. Documentar mejores prÃ¡cticas

---

**Inteligencia artificial avanzada para automatizaciÃ³n inteligente** ğŸ¤–âœ¨